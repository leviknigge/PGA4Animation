{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\levik\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# import wandb\n",
    "\n",
    "# import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def build_path(path):\n",
    "    for i in path:\n",
    "        if not os.path.exists(i):\n",
    "            os.makedirs(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45002, 3041) (45004, 186)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "\n",
    "imax = 381294\n",
    "\n",
    "skiprows = 2\n",
    "max_rows = 1000\n",
    "goal_rows = 50000-5000\n",
    "\n",
    "x = np.loadtxt(\"generate/new_algebra_input_100000.txt\",delimiter=\",\",max_rows=2)\n",
    "\n",
    "for i in range(int(goal_rows/max_rows)):\n",
    "    test = np.loadtxt(\"generate/new_algebra_input_100000.txt\",delimiter=\",\",max_rows=max_rows,skiprows=int(1+i*max_rows))\n",
    "    x = np.concatenate((x,test))\n",
    "test=0\n",
    "\n",
    "    \n",
    "y = np.loadtxt(\"generate/new_algebra_output_100000.txt\",delimiter=\",\",max_rows=2*2)\n",
    "\n",
    "for i in range(int((goal_rows/max_rows))):\n",
    "#     print(max_rows)\n",
    "#     print(int(3+i*max_rows))\n",
    "#     print(i,(1+i*max_rows)*2)\n",
    "    test1 = np.loadtxt(\"generate/new_algebra_output_100000.txt\",delimiter=\",\",max_rows=max_rows,skiprows=int(1+i*max_rows))\n",
    "#     print(y.shape,test.shape)\n",
    "    y = np.concatenate((y,test1))\n",
    "\n",
    "print(x.shape,y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-73f93d9057c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Creating data indices for training and validation splits:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdataset_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0msplit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_split\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# split into train and test\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(x)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "# # print(train_indices)\n",
    "# # print(val_indices)\n",
    "# # print(len(train_indices), len(val_indices))\n",
    "# train_sampler = SubsetRandomSampler(train_indices)\n",
    "# valid_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "\n",
    "Xmean, Xstd = x.mean(axis=0), x.std(axis=0)\n",
    "Ymean, Ystd = y.mean(axis=0), y.std(axis=0)\n",
    "\n",
    "# print(Xmean, Xstd)\n",
    "# print(Ymean, Ystd)\n",
    "\n",
    "\n",
    "for i in range(Xstd.size):\n",
    "    if (Xstd[i]==0):\n",
    "        Xstd[i]=1\n",
    "for i in range(Ystd.size):\n",
    "    if (Ystd[i]==0):\n",
    "        Ystd[i]=1\n",
    "        \n",
    "        \n",
    "x = (x - Xmean) / Xstd\n",
    "y = (y - Ymean) / Ystd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the normalization values\n",
    "# np.savetxt(\"Xmean_algebra_100000_11.txt\",Xmean,delimiter=\",\",fmt=\"%1.10f\")\n",
    "# np.savetxt(\"Xstd_algebra_100000_11.txt\",Xstd,delimiter=\",\",fmt=\"%1.10f\")\n",
    "# np.savetxt(\"Ymean_algebra_100000_11.txt\",Ymean,delimiter=\",\",fmt=\"%1.10f\")\n",
    "# np.savetxt(\"Xstd_algebra_100000_11.txt\",Ystd,delimiter=\",\",fmt=\"%1.10f\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually split the dataset\n",
    "xtrain = x[train_indices]\n",
    "ytrain = y[train_indices]\n",
    "\n",
    "xtest = x[test_indices]\n",
    "ytest = y[test_indices]\n",
    "\n",
    "Xdim = xtrain.shape[1]\n",
    "Ydim = ytrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network tensors\n",
    "\n",
    "x = torch.tensor(xtrain)\n",
    "y = torch.tensor(ytrain)\n",
    "\n",
    "testx = torch.tensor(xtest)\n",
    "testy = torch.tensor(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12002, 3041])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12002, 186])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight initializations\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.normal_(m.bias, 0.0, 0.001)\n",
    "        torch.nn.init.zeros_(m.weight)\n",
    "\n",
    "# tat = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
    "# tat.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the torch network\n",
    "\n",
    "x, y = Variable(x), Variable(y)\n",
    "testx, testy = Variable(testx), Variable(testy)\n",
    "\n",
    "hidden_size = 512\n",
    "dropout_p = 0.3\n",
    "hidden_layers = 2\n",
    "activation = \"ELU\"\n",
    "\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "        \n",
    "        torch.nn.LayerNorm(Xdim),\n",
    "        torch.nn.Dropout(p=dropout_p),\n",
    "\n",
    "    \n",
    "        torch.nn.Linear(Xdim, hidden_size),\n",
    "        torch.nn.LayerNorm(hidden_size),\n",
    "    \n",
    "        torch.nn.ELU(),\n",
    "        torch.nn.Dropout(p=dropout_p),\n",
    "    \n",
    "        torch.nn.Linear(hidden_size, hidden_size),\n",
    "        torch.nn.LayerNorm(hidden_size),\n",
    "    \n",
    "        torch.nn.ELU(),\n",
    "        torch.nn.Dropout(p=dropout_p),\n",
    "    \n",
    "    \n",
    "#         torch.nn.Linear(hidden_size, hidden_size),\n",
    "#         torch.nn.LayerNorm(hidden_size),\n",
    "\n",
    "#         torch.nn.ELU(),\n",
    "#         torch.nn.Dropout(p=dropout_p),\n",
    "        \n",
    "        torch.nn.Linear(hidden_size, Ydim),\n",
    "    )\n",
    "\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "epochs = 20\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.025\n",
    "momentum = 0.9\n",
    "optim = \"SGD\"\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "def cross_entropy(Y, H3):\n",
    "    return \n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "torch_dataset = Data.TensorDataset(x,y)\n",
    "test_dataset = Data.TensorDataset(x,y)\n",
    "\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, num_workers=2,)\n",
    "\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=torch_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, num_workers=2,)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "#                                            sampler=train_sampler)\n",
    "# validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "#                                                 sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb for tracking the training process\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%wandb\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(16,10))\n",
    "wandb.init(project=\"pytorch_motions_algebra\", config={\"learning-rate\": learning_rate, \"epochs\":epochs, \n",
    "                                                      \"batch-size\": BATCH_SIZE, \"weight-decay\":weight_decay,\n",
    "                                                      \"hidden-size\":hidden_size, \"dropout-p\":dropout_p,\n",
    "                                                      \"hidden-layers\":hidden_layers,\"activation\":activation,\n",
    "                                                      \"random-seed\":random_seed, \"validation-split\":validation_split,\n",
    "                                                      \"dataset-size\":dataset_size, \"optimizer\":optim\n",
    "                                                     }, reinit=True)\n",
    "\n",
    "# Training process\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_steps = 0\n",
    "    test_steps = 0\n",
    "    net.train()\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y).float()\n",
    "\n",
    "        prediction = net(b_x.float())\n",
    "        \n",
    "#         print(prediction.shape,b_y.shape)\n",
    "        \n",
    "        loss = loss_func(prediction, b_y)\n",
    "        \n",
    "        train_loss = train_loss + loss\n",
    "        train_steps = step\n",
    "        \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step() \n",
    "#     print(loss.item())\n",
    "        \n",
    "        \n",
    "    train_loss = train_loss / train_steps\n",
    "    wandb.log({\"train_loss\": train_loss})\n",
    "    \n",
    "    net.eval()\n",
    "    for step, (batch_x, batch_y) in enumerate(test_loader):\n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y).float()\n",
    "\n",
    "        prediction = net(b_x.float())\n",
    "        \n",
    "#         print(prediction.shape,b_y.shape)\n",
    "        \n",
    "        loss = loss_func(prediction, b_y)\n",
    "        \n",
    "        test_loss = test_loss + loss\n",
    "        test_steps = step\n",
    "\n",
    "    test_loss = test_loss / test_steps\n",
    "    wandb.log({\"test_loss\": test_loss})\n",
    "    \n",
    "    print('Epoch:',  '%04d' % (epoch + 1), 'trainingloss =', train_loss)\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'testloss =', test_loss)\n",
    "        \n",
    "print('Done!')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b2f5ff895fa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"model_agebra_100000.pt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# print(net)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"model_agebra_100000.pt\"\n",
    "torch.save(net.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the network\n",
    "\n",
    "hidden_size = 1024\n",
    "dropout_p = 0.3\n",
    "hidden_layers = 3\n",
    "activation = \"ELU\"\n",
    "# BATCH_SIZE = 64\n",
    "epochs = 20\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.025\n",
    "momentum = 0.9\n",
    "optim = \"SGD\"\n",
    "\n",
    "\n",
    "path = \"model_algebra_100000.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "        \n",
    "        torch.nn.LayerNorm(Xdim,\n",
    "        torch.nn.Dropout(p=dropout_p),\n",
    "\n",
    "    \n",
    "        torch.nn.Linear(Xdim, hidden_size),\n",
    "        torch.nn.LayerNorm(hidden_size),\n",
    "    \n",
    "        torch.nn.ELU(),\n",
    "        torch.nn.Dropout(p=dropout_p),\n",
    "    \n",
    "        torch.nn.Linear(hidden_size, hidden_size),\n",
    "        torch.nn.LayerNorm(hidden_size),\n",
    "    \n",
    "        torch.nn.ELU(),\n",
    "        torch.nn.Dropout(p=dropout_p),\n",
    "    \n",
    "    \n",
    "        torch.nn.Linear(hidden_size, hidden_size),\n",
    "        torch.nn.LayerNorm(hidden_size),\n",
    "\n",
    "        torch.nn.ELU(),\n",
    "        torch.nn.Dropout(p=dropout_p),\n",
    "        \n",
    "        torch.nn.Linear(hidden_size, Ydim),\n",
    "    )\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing validation data\n",
    "\n",
    "val_y = np.array(pd.read_csv(\"generate/algebra_output_100000_4.txt\", header=None, skiprows=0))\n",
    "val_x = np.array(pd.read_csv(\"generate/algebra_input_100000_4.txt\", header=None, skiprows=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = np.loadtxt(\"generate/new_algebra_input_100000.txt\",delimiter=\",\",skiprows=29722,max_rows=501)\n",
    "val_y = np.loadtxt(\"generate/new_algebra_output_100000.txt\",delimiter=\",\",skiprows=29722,max_rows=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_y = np.array(pd.read_csv(\"algebra_output_0.txt\", header=None, skiprows=0, nrows=600))\n",
    "# val_x = np.array(pd.read_csv(\"algebra_input_0.txt\", header=None, skiprows=0, nrows=600))\n",
    "# print(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = (val_x - Xmean) / Xstd\n",
    "val_y = (val_y - Ymean) / Ystd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501, 186)\n",
      "(501, 186)\n"
     ]
    }
   ],
   "source": [
    "var_x = Variable(torch.tensor(np.array(val_x)))\n",
    "\n",
    "prediction = model(var_x.float())\n",
    "\n",
    "output = np.around(np.array((prediction.data * Ystd) + Ymean), 6)\n",
    "val_y = np.around(np.array((val_y.data *Ystd) + Ymean), 6)\n",
    "\n",
    "print(val_y.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.12000e-04 -7.40000e-05  5.34400e-02 -4.95010e-02  2.06380e-02\n",
      "  1.32245e-01]\n",
      "[ 0.        0.        0.055103 -0.052519  0.020292  0.110004]\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "j = 10\n",
    "\n",
    "print(output[i, j:j+6])\n",
    "print(val_y[i,j:j+6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.        0.        0.055103 -0.052519  0.020292  0.110004]\n"
     ]
    }
   ],
   "source": [
    "print(val_y[i,j:j+6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.393918\n"
     ]
    }
   ],
   "source": [
    "print(output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/verybad_animation.txt\", \"w+\") as f:\n",
    "    for row in output:\n",
    "        f.write(\",\".join(format(x, \"1.6f\") for x in row) + \"\\n\")\n",
    "        \n",
    "with open(\"results/verygood_animation.txt\", \"w+\") as f:\n",
    "    for row in val_y:\n",
    "        f.write(\",\".join(format(x, \"1.6f\") for x in row) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
